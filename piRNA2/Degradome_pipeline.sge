## SGE script for running Degradome mapping on cluster
# also works for mouse_adult_wt_Degradome.polyA_100nt_strand data
# Author: Xianjun Dong
# Date: 2012.04.9
# Usage: qsub Degradome_pipeline.sge /home/dongx/nearline/Xin/others/mouse_12.5dpp_wt_Degradome mouse_12.5dpp_wt_Degradome

#!/bin/bash
#$ -V
#$ -pe openmpi 8
#$ -cwd
#$ -o $HOME/sge_jobs_output/sge_job.$JOB_ID.out -j y
#$ -S /bin/bash
#$ -l mem_free=4G

INPUTDIR=$1
index='mm9'
OUTPUTDIR="$HOME/scratch/$2"  #"mouse_adult_wt_CAGE_PE100nt_strand"
[ -d $OUTPUTDIR ] || mkdir -p $OUTPUTDIR
ln -fs $HOME/sge_jobs_output/sge_job.$JOB_ID.out $OUTPUTDIR/sge.log
OUTPUTfile=$OUTPUTDIR/$2

stat_file=$OUTPUTDIR/mapping.stat

export BOWTIE_INDEXES=$GENOME/$index/Sequence/BowtieIndex/
export BOWTIE2_INDEXES=$GENOME/$index/Sequence/Bowtie2Index/
export ANNOTATION=$GENOME/$index/Annotation/Genes

#==============================================
# ---------- quality filtern ---------------
#==============================================

# 1. filter bad reads (from Illumina) and merge all individual files into one file
# only merge R1 (Note: for PE reads, we cannot use the ':Y:' tag to filter bad reads, because the bad reads is decided if either one is bad)
#zcat $INPUTDIR/*_R1*.fastq.gz > $INPUTDIR/filtered.fastq

phred=`getphred $INPUTDIR/filtered.fastq`; tophat_scoreoption=""; # default
[ "$phred" == "Phred+64" ] && tophat_scoreoption="--solexa1.3-quals";
[ "$phred" == "Solexa+64" ] && tophat_scoreoption="--solexa-quals";
echo "tophat_scoreoption: $tophat_scoreoption"

echo "Total number of reads: "`wc -l $INPUTDIR/filtered.fastq | awk '{printf("%'\''d",$1);}'` > $stat_file
# 2. remove adaptor
#flexbar -t $INPUTDIR/filtered.trimmed -f fastq-sanger -s $INPUTDIR/filtered.fastq -n 8 -as $adaptorsequence -at 3 -ao 10 --min-readlength 20 --max-uncalled 70 --phred-pre-trim 10

#echo "After adaptor removal: "`wc -l $INPUTDIR/filtered.trimmed.fastq |  awk '{printf("%'\''d",$1);}'` >> $stat_file

# 3. for reads beginning with NG/GG, trim the heading 2nt; if the remain length >20nt, then output
#cat $INPUTDIR/filtered.trimmed.fastq | perl -ne '$h=$_; $s=<>; chomp($s); $id=<>; $t=<>; chomp($t); if($s=~/^[NG]G/) {$s=~s/(^[NG]G)//; $t=substr($t, length($1)); if(length($s)>20) {print "$h$s\n$id$t\n";}}' > $OUTPUTfile.fastq
#cat $INPUTDIR/filtered.fastq | perl -ne '$h=$_; $s=<>; chomp($s); $id=<>; $t=<>; chomp($t); if($s=~/^[NG]G/) {$s=~s/(^[NG]G)//; $t=substr($t, length($1)); if(length($s)>20) {print "$h$s\n$id$t\n";}}' > $OUTPUTfile.fastq
#
#echo "After trimming/filtering: "`wc -l $OUTPUTfile.fastq |  awk '{printf("%'\''d",$1);}'` >> $stat_file
ln -sf $INPUTDIR/filtered.fastq input.fastq
#==============================================
# ------------- mapping
#==============================================
# using tophat
$HOME/bin/tophat-2.0.4.Linux_x86_64/tophat -o $OUTPUTDIR $tophat_scoreoption --no-convert-bam -p 8 --read-mismatches 2 --library-type fr-secondstrand --min-anchor-length 8 --min-intron-length 30 --max-intron-length 50000 --splice-mismatches 1 --max-multihits 100 --no-coverage-search genome input.fastq

mv $OUTPUTDIR/accepted_hits.sam $OUTPUTfile.sam

cut -f1 $OUTPUTfile.sam | sort | uniq -c > $OUTPUTfile.sam.IDsorted.temp
echo "Mapped reads: "`wc -l $OUTPUTfile.sam.IDsorted.temp |  awk '{printf("%'\''d",$1);}'` >> $stat_file
echo "  among which, unique mapper and multi-mappers:" >> $stat_file
textHistogram -maxBinCount=2 -noStar $OUTPUTfile.sam.IDsorted.temp >> $stat_file

#==============================================
## ---------- USCS display
#==============================================

# ------------- track for unique mapped reads

awk '{for(i=1;i<=NF;i++) if($i~/NH:i:1\t/ || $i~/NH:i:1$/) print}' $OUTPUTfile.sam | sam2bed -v bed12=T > $OUTPUTfile.unique.bed
bam2bw $OUTPUTfile.unique.bed

# ------------- track for all mapped reads
bam2bw $OUTPUTfile.sam

cp -r $OUTPUTfile*.bw ~/scratch/ucsc

echo "JOB DONE";
